#!/usr/bin/env python
# Copyright 2025 the LlamaFactory team.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json
import os
import re
import time
import traceback
from concurrent.futures import ProcessPoolExecutor, as_completed
from statistics import mean
from typing import List, Optional, Tuple, Set, Union

import fire
from datasets import load_dataset
from tqdm import tqdm

# å…¨å±€å˜é‡ï¼Œç”¨äºåœ¨å­è¿›ç¨‹ä¸­å»¶è¿ŸåŠ è½½åº“
evaluate = None
torch = None


# ===================================================================================
# ==               æ¨¡å—ä¸€: å•ä¸ªæ–‡ä»¶çš„è¯„ä¼°é€»è¾‘ (ç”±å·¥ä½œè¿›ç¨‹è°ƒç”¨)                     ==
# ===================================================================================

def _lazy_import():
    """å»¶è¿Ÿå¯¼å…¥cudaæ•æ„Ÿåº“ï¼Œç¡®ä¿åœ¨è®¾ç½®CUDA_VISIBLE_DEVICESåæ‰§è¡Œã€‚"""
    global evaluate, torch
    if evaluate is None:
        import evaluate as _evaluate
        evaluate = _evaluate
    if torch is None:
        import torch as _torch
        torch = _torch


def _extract_answer(text: str, dataset_type: str) -> Optional[str]:
    """æ ¹æ®æ•°æ®é›†ç±»å‹ä»å•ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²ä¸­æå–ç­”æ¡ˆçš„è¾…åŠ©å‡½æ•°ã€‚"""
    if not isinstance(text, str) or not text.strip(): return None
    text = text.strip()
    if dataset_type == "logiqa":
        match = re.search(r"[Aa]nswer:\s*([A-D])", text, re.IGNORECASE) or re.search(r"\b([A-D])\b\s*$", text, re.IGNORECASE)
        return match.group(1).upper() if match else None
    if dataset_type in ["gsm8k", "metamathqa"]:
        if (last_marker_pos := text.rfind('####')) != -1:
            answer_text = text[last_marker_pos + 4:]
            if (match := re.search(r"-?[\d,]+\.?\d*|-?\d+\.?[\d,]*", answer_text)): return match.group(0).replace(",", "").strip()
        if (matches := re.findall(r"\\boxed{([^}]+)}", text)):
            answer_text = matches[-1]
            if (match := re.search(r"-?[\d,]+\.?\d*|-?\d+\.?[\d,]*", answer_text)): return match.group(0).replace(",", "").strip()
            return answer_text.strip()
        if (matches := re.findall(r"(?:[Tt]he answer is|[Aa]nswer:)\s*\$?(-?[\d,./]+)", text)): return matches[-1].replace(",", "").replace("$", "").strip()
        if not matches and last_marker_pos == -1:
            if (match := re.search(r"(-?[\d,./]+)\s*$", text)): return match.group(1).replace(",", "").strip()
        return None
    return text.strip()


def calculate_exact_match(predictions: List[str], references: List[str], dataset_type: str) -> List[float]:
    """è®¡ç®—ç²¾ç¡®åŒ¹é…åˆ†æ•°ï¼Œå¸¦tqdmè¿›åº¦æ¡ã€‚"""
    em_scores = []
    for pred_str, label_str in tqdm(zip(predictions, references), desc=f"  EM for '{dataset_type}'", total=len(predictions), leave=False, ncols=80, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}'):
        em_score = 0.0
        pred_ans, label_ans = _extract_answer(pred_str, dataset_type), _extract_answer(label_str, dataset_type)
        if pred_ans is not None and label_ans is not None:
            try:
                if abs(float(pred_ans) - float(label_ans)) < 1e-6: em_score = 1.0
            except (ValueError, TypeError):
                if pred_ans.strip().lower() == label_ans.strip().lower(): em_score = 1.0
        em_scores.append(em_score)
    return em_scores


def run_single_evaluation(filename: str, output_filename: str, enabled_metrics: Set[str]) -> dict:
    """å¯¹å•ä¸ªæ–‡ä»¶è¿›è¡Œè¯„ä¼°ï¼Œæ”¯æŒåˆ›å»ºæˆ–æ›´æ–°(è¿½åŠ )æ¨¡å¼ã€‚"""
    _lazy_import()
    if os.path.exists(output_filename):
        try:
            with open(output_filename, 'r', encoding='utf-8') as f: average_score = json.load(f)
        except json.JSONDecodeError:
            return {"status": "failed", "reason": f"Corrupted JSON in existing metrics file: {os.path.basename(output_filename)}"}
    else:
        average_score = {}

    try:
        dataset = list(load_dataset("json", data_files=filename, split="train"))
    except Exception as e:
        return {"status": "failed", "reason": f"Failed to load dataset: {e}", "traceback": traceback.format_exc()}
    
    if not dataset: return {"status": "skipped", "reason": "empty file"}

    predictions = [sample.get("predict") for sample in dataset]
    references = [sample.get("label") for sample in dataset]
    if None in predictions or None in references:
        return {"status": "skipped", "reason": "missing 'predict' or 'label' field"}

    if "bertscore" in enabled_metrics:
        bertscore = evaluate.load("bertscore", keep_in_memory=True)
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        bertscore_results = bertscore.compute(predictions=predictions, references=references, lang="en", model_type="bert-base-uncased", device=device, batch_size=32, verbose=False)
        average_score["bertscore-f1"] = round(mean(bertscore_results["f1"]) * 100, 4)

    if "rouge" in enabled_metrics:
        rouge = evaluate.load("rouge", keep_in_memory=True)
        rouge_scores = rouge.compute(predictions=predictions, references=references)
        average_score.update({"rouge-1": round(rouge_scores["rouge1"] * 100, 4), "rouge-2": round(rouge_scores["rouge2"] * 100, 4), "rouge-l": round(rouge_scores["rougeL"] * 100, 4), "rouge-Lsum": round(rouge_scores["rougeLsum"] * 100, 4)})

    if "bleu" in enabled_metrics:
        bleu = evaluate.load("bleu", keep_in_memory=True)
        bleu_scores = bleu.compute(predictions=predictions, references=[[ref] for ref in references])
        average_score["bleu"] = round(bleu_scores["bleu"] * 100, 4)

    if "em" in enabled_metrics:
        fn_lower, dataset_type = filename.lower(), "default"
        if "logiqa" in fn_lower: dataset_type = "logiqa"
        elif "gsm8k" in fn_lower: dataset_type = "gsm8k"
        elif "metamath" in fn_lower: dataset_type = "metamathqa"
        em_scores = calculate_exact_match(predictions, references, dataset_type)
        average_score["exact_match"] = round(mean(em_scores) * 100, 4)

    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(average_score, f, indent=4, ensure_ascii=False)
    
    return {"status": "success", "scores": average_score}


# ===================================================================================
# ==               æ¨¡å—äºŒ: å·¥ä½œè¿›ç¨‹å°è£… (ç”±ä¸»è¿›ç¨‹åˆ›å»º)                             ==
# ===================================================================================

def worker_evaluate(task_args: Tuple[str, str, int, str, Set[str]]) -> dict:
    """å·¥ä½œè¿›ç¨‹æ‰§è¡Œçš„å‡½æ•°ã€‚"""
    input_path, output_path, gpu_id, root_dir, enabled_metrics = task_args
    if gpu_id != -1: os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    start_time = time.time()
    relative_path = os.path.relpath(input_path, root_dir)
    try:
        result = run_single_evaluation(input_path, output_path, enabled_metrics)
    except Exception as e:
        result = {"status": "failed", "reason": "Unhandled exception in worker", "error": str(e), "traceback": traceback.format_exc()}
    result['elapsed'] = time.time() - start_time
    result['path'] = relative_path
    return result


# ===================================================================================
# ==               æ¨¡å—ä¸‰: ä¸»è¿›ç¨‹ä¸ä»»åŠ¡ç¼–æ’ (è„šæœ¬å…¥å£)                             ==
# ===================================================================================

def process_all_results(
    root_dir: str = "/home/yijiexu/LLaMA-Factory/results",
    num_workers: int = 12,
    gpus: Union[str, Tuple[str]] = "0,1,2,3,4,5",
    metric: str = "all",
    force: bool = False
):
    """
    å¹¶è¡Œè¯„ä¼°æ‰€æœ‰ç»“æœæ–‡ä»¶ï¼Œæ”¯æŒè®¡ç®—å…¨éƒ¨æŒ‡æ ‡æˆ–è¿½åŠ å•ä¸ªæŒ‡æ ‡ã€‚

    Args:
        root_dir (str): å®éªŒç»“æœçš„æ ¹ç›®å½•ã€‚
        num_workers (int): å¹¶è¡Œå¤„ç†æ–‡ä»¶çš„è¿›ç¨‹æ•°ã€‚
        gpus (Union[str, Tuple[str]]): ä»¥é€—å·åˆ†éš”çš„GPU IDåˆ—è¡¨ã€‚
        metric (str): è¦è®¡ç®—çš„æŒ‡æ ‡ã€‚å¯é€‰: "all", "bertscore"ã€‚
        force (bool): å¦‚æœä¸ºTrueï¼Œå³ä½¿æŒ‡æ ‡å·²å­˜åœ¨ä¹Ÿå¼ºåˆ¶é‡æ–°è®¡ç®—ã€‚
    """
    print(f"--- å¼€å§‹å¹¶è¡Œæ‰¹é‡è¯„ä¼° (æ¨¡å¼: {metric}) ---")
    
    # *** å…³é”®ä¿®å¤ï¼šå¥å£®åœ°å¤„ç† gpus å‚æ•° ***
    gpu_ids = []
    if gpus:
        gpus_str = ""
        if isinstance(gpus, tuple):
            gpus_str = ",".join(map(str, gpus))
        elif isinstance(gpus, (str, int)):
            gpus_str = str(gpus)

        if gpus_str:
            try:
                gpu_ids = [int(g.strip()) for g in gpus_str.split(',')]
            except ValueError:
                print(f"é”™è¯¯: 'gpus' å‚æ•°æ ¼å¼ä¸æ­£ç¡®ã€‚æ”¶åˆ°äº† '{gpus}'ã€‚åº”ä¸ºé€—å·åˆ†éš”çš„æ•´æ•°ï¼Œä¾‹å¦‚ '0,1,2'ã€‚")
                return
            
    print(f"é…ç½®: æ ¹ç›®å½•='{root_dir}', å·¥ä½œè¿›ç¨‹æ•°={num_workers}, å¯ç”¨GPU={gpu_ids if gpu_ids else 'CPU'}, å¼ºåˆ¶é‡ç®—={force}")

    if metric.lower() == "all":
        enabled_metrics = {"bleu", "rouge", "bertscore", "em"}
        metric_keys_to_check = [] 
    elif metric.lower() == "bertscore":
        enabled_metrics = {"bertscore"}
        metric_keys_to_check = ["bertscore-f1"]
    else:
        print(f"é”™è¯¯: ä¸æ”¯æŒçš„æŒ‡æ ‡ '{metric}'ã€‚è¯·é€‰æ‹© 'all' æˆ– 'bertscore'ã€‚"); return

    print("\n[1/3] æ­£åœ¨æ‰«ææ–‡ä»¶å¹¶å‡†å¤‡ä»»åŠ¡...")
    all_potential_files, tasks_to_run, skipped_count = [], [], 0
    for dirpath, _, filenames in os.walk(root_dir):
        for filename in filenames:
            basename, _ = os.path.splitext(filename)
            if filename.endswith(".jsonl") and not (basename.endswith("_metrics") or basename.endswith("_LLM_EM")):
                all_potential_files.append(os.path.join(dirpath, filename))
    all_potential_files.sort()
    
    for input_path in all_potential_files:
        basename, _ = os.path.splitext(input_path)
        output_path = f"{basename}_metrics.json"

        if not force:
            if not os.path.exists(output_path) and metric_keys_to_check:
                skipped_count += 1; continue
            if os.path.exists(output_path) and metric_keys_to_check:
                try:
                    with open(output_path, 'r', encoding='utf-8') as f: existing_data = json.load(f)
                    if all(key in existing_data for key in metric_keys_to_check):
                        skipped_count += 1; continue
                except (json.JSONDecodeError, FileNotFoundError): pass
            elif os.path.exists(output_path) and not metric_keys_to_check:
                skipped_count += 1; continue

        assigned_gpu = gpu_ids[len(tasks_to_run) % len(gpu_ids)] if gpu_ids else -1
        tasks_to_run.append((input_path, output_path, assigned_gpu, root_dir, enabled_metrics))
    
    if not tasks_to_run:
        print(f"æ‰«æå®Œæˆã€‚æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–°æ–‡ä»¶ã€‚è·³è¿‡ {skipped_count} ä¸ªå·²å®Œæˆ/ä¸é€‚ç”¨çš„ä»»åŠ¡ã€‚"); return
    print(f"æ‰«æå®Œæˆã€‚æ‰¾åˆ° {len(tasks_to_run)} ä¸ªæ–°ä»»åŠ¡ã€‚è·³è¿‡ {skipped_count} ä¸ªå·²å®Œæˆ/ä¸é€‚ç”¨çš„ä»»åŠ¡ã€‚")

    print("\n[2/3] å¼€å§‹å¹¶è¡Œå¤„ç†ä»»åŠ¡...")
    success_count, fail_count, skipped_in_run_count = 0, 0, 0
    failed_tasks_details = []
    overall_start_time = time.time()
    
    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        futures = {executor.submit(worker_evaluate, task): task for task in tasks_to_run}
        pbar = tqdm(total=len(tasks_to_run), desc="Overall Progress", ncols=120, dynamic_ncols=True)
        for future in as_completed(futures):
            try:
                result = future.result()
                status, path_str = result['status'], result.get('path', 'unknown_file')
                if status == 'failed':
                    fail_count += 1; status_char = "âŒ"; failed_tasks_details.append(result)
                    pbar.write("\n" + "!"*35 + " TASK FAILED " + "!"*34)
                    pbar.write(f"  [File]: {result['path']}\n  [Reason]: {result.get('reason') or result.get('error')}")
                    if 'traceback' in result and result['traceback']: pbar.write(f"  [Traceback]:\n{result['traceback']}")
                    pbar.write("!"*80 + "\n")
                elif status == 'success': success_count += 1; status_char = "âœ…"
                elif status == 'skipped': skipped_in_run_count += 1; status_char = "â©"
                pbar.set_postfix_str(f"Success: {success_count}, Failed: {fail_count}, Last: {path_str} {status_char}", refresh=True)
            except Exception as e:
                task = futures[future]
                relative_path = os.path.relpath(task[0], task[3])
                fail_count += 1; tb_str = traceback.format_exc(); failed_tasks_details.append({"status": "crashed", "path": relative_path, "error": str(e), "traceback": tb_str})
                pbar.write(f"\n" + "!"*33 + " WORKER CRASHED " + "!"*32 + f"\n  [File]: {relative_path}\n  [Error]: {e}\n  [Traceback]:\n{tb_str}\n" + "!"*80 + "\n")
                pbar.set_postfix_str(f"Success: {success_count}, Failed: {fail_count}, Last: {relative_path} ğŸ”¥CRASHEDğŸ”¥", refresh=True)
            pbar.update(1)
        pbar.close()

    overall_end_time = time.time()
    print("\n\n[3/3] æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæ¯•ï¼Œç”Ÿæˆæ‘˜è¦æŠ¥å‘Šã€‚")
    print("#"*80)
    print("###" + " " * 30 + "æ‰¹é‡è¯„ä¼°ä»»åŠ¡å®Œæˆï¼" + " " * 29 + "###")
    print("#"*80)
    print(f"æ€»è®¡ç”¨æ—¶: {overall_end_time - overall_start_time:.2f} ç§’")
    print(f"  - æˆåŠŸå¤„ç†/æ›´æ–°æ–‡ä»¶æ•°: {success_count} âœ…")
    print(f"  - å¤±è´¥æ–‡ä»¶æ•°: {fail_count} âŒ")
    print(f"  - å› æ–‡ä»¶å†…å®¹é—®é¢˜è·³è¿‡çš„æ–‡ä»¶æ•°: {skipped_in_run_count} â©")
    print(f"  - å› æŒ‡æ ‡å·²å­˜åœ¨/ä¸é€‚ç”¨è€Œè·³è¿‡çš„æ–‡ä»¶æ•°: {skipped_count} â­ï¸")
    if failed_tasks_details:
        print("\n--- å¤±è´¥ä»»åŠ¡ç®€è¦å›é¡¾ (è¯¦ç»†æ—¥å¿—è§ä¸Šæ–¹) ---")
        for i, failure in enumerate(failed_tasks_details, 1): print(f"  {i}. {failure['path']} ({failure['status']})")
    print("#"*80)


if __name__ == "__main__":
    fire.Fire(process_all_results)